import pandas as pd
import torch
import torch.nn as nn
import matplotlib.pyplot as plt
from sklearn.model_selection import KFold
from torch.utils.data import DataLoader, TensorDataset
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, precision_recall_curve, auc
import os
from scipy.stats import wilcoxon
import numpy as np
import random
from tqdm import tqdm  # Import tqdm for progress bars

# Load tokenizer
model_name = "roberta-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Load dataset
df = pd.read_csv('/kaggle/input/augmented-dataset/augmented-dataset')

# Ensure augmentation is applied only to training data
def augment_text(text):
    return text  # Replace with actual augmentation logic

df['content'] = df['content'].apply(augment_text)

# Tokenize Data
def encode_data(text_list):
    return tokenizer.batch_encode_plus(
        text_list.astype(str).tolist(),
        add_special_tokens=True,
        return_attention_mask=True,
        max_length=100,
        return_tensors='pt',
        padding='max_length',
        truncation=True
    )

encoded_data = encode_data(df['content'])
input_ids = encoded_data['input_ids']
attention_masks = encoded_data['attention_mask']
labels = torch.tensor(df['labels'].to_numpy(), dtype=torch.long)

# Dataset Preparation
dataset = TensorDataset(input_ids, attention_masks, labels)

# Model Definition
class CustomXLNet(nn.Module):
    def __init__(self, model_name, num_labels):
        super(CustomXLNet, self).__init__()
        self.xlnet = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)
        self.dropout = nn.Dropout(0.3)

    def forward(self, input_ids, attention_mask, labels=None):
        outputs = self.xlnet(input_ids, attention_mask=attention_mask)
        logits = self.dropout(outputs.logits)
        return logits

# Loss function and device
loss_fn = nn.CrossEntropyLoss()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Hyperparameters
num_labels = 7
batch_size = 10
num_epochs = 10
k_folds = 5
patience = 3  # Early stopping patience

# Randomly generate seeds
num_seeds = 2  # Number of seeds you want to generate
seeds = [random.randint(0, 1000) for _ in range(num_seeds)]  # Random seeds between 0 and 1000
print("Randomly Generated Seeds:", seeds)  # Print the seeds for reference

# Creating output directory
output_dir = "kaggle/working"
os.makedirs(output_dir, exist_ok=True)

# Storing results for each seed
all_results = []
all_train_losses = []
all_val_losses = []
all_test_metrics = []

for seed in seeds:
    kf = KFold(n_splits=k_folds, shuffle=True, random_state=seed)
    fold_results = []
    
    # Initialize list to store per-fold per-emotion metrics for this seed
    seed_emotion_fold_metrics = []
    # New Code Block: Create lists to store epoch losses/metrics per fold for this seed
    seed_train_losses = []
    seed_val_losses = []
    seed_test_metrics = []

    for fold, (train_idx, val_idx) in enumerate(kf.split(input_ids)):
        print(f"Seed {seed} - Fold {fold + 1}/{k_folds}")

        # Create DataLoaders
        train_subsampler = torch.utils.data.Subset(dataset, train_idx)
        val_subsampler = torch.utils.data.Subset(dataset, val_idx)

        train_loader = DataLoader(train_subsampler, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_subsampler, batch_size=batch_size, shuffle=False)

        # Model Initialization
        model = CustomXLNet(model_name, num_labels).to(device)
        optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)
        
        train_losses = []
        val_losses = []
        test_metrics = []

        # Early stopping variables
        best_val_loss = float('inf')
        epochs_no_improve = 0
        best_model_state = None

        for epoch in range(num_epochs):
            model.train()
            total_train_loss = 0.0
            total_val_loss = 0.0

            # Training Phase with tqdm
            with tqdm(train_loader, desc=f"Epoch {epoch + 1}/{num_epochs} - Training", unit="batch") as t:
                for batch_idx, batch in enumerate(t):
                    input_ids_batch, attention_masks_batch, labels_batch = [t.to(device) for t in batch]
                    optimizer.zero_grad()
                    outputs = model(input_ids_batch, attention_mask=attention_masks_batch)
                    loss = loss_fn(outputs, labels_batch)
                    loss.backward()
                    optimizer.step()
                    total_train_loss += loss.item()

                    # Update tqdm postfix with current loss
                    t.set_postfix(loss=loss.item())

            # Log training loss for the epoch
            train_losses.append(total_train_loss / len(train_loader))

            # Validation Phase with tqdm
            model.eval()
            all_preds, all_labels, all_probs = [], [], []
            
            with torch.no_grad():
                with tqdm(val_loader, desc=f"Epoch {epoch + 1}/{num_epochs} - Validation", unit="batch") as v:
                    for batch in v:
                        input_ids_batch, attention_masks_batch, labels_batch = [t.to(device) for t in batch]
                        outputs = model(input_ids_batch, attention_mask=attention_masks_batch)
                        probs = torch.softmax(outputs, dim=1)  # Get probabilities for ROC-AUC and PR-AUC
                        _, predicted = torch.max(outputs, 1)
                        
                        all_preds.extend(predicted.cpu().numpy())
                        all_labels.extend(labels_batch.cpu().numpy())
                        all_probs.extend(probs.cpu().numpy())
            
            # Convert list of arrays to a single 2D array
            all_probs_array = np.vstack(all_probs)

            # Calculate metrics
            accuracy = accuracy_score(all_labels, all_preds)
            precision = precision_score(all_labels, all_preds, average='weighted')
            recall = recall_score(all_labels, all_preds, average='weighted')
            f1 = f1_score(all_labels, all_preds, average='weighted')
            roc_auc = roc_auc_score(all_labels, all_probs_array, multi_class='ovr', average='weighted')
            precision_pr, recall_pr, _ = precision_recall_curve(all_labels, all_probs_array[:, 1], pos_label=1)
            pr_auc = auc(recall_pr, precision_pr)

            # Store metrics for this epoch
            test_metrics.append({
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1': f1,
                'roc_auc': roc_auc,
                'pr_auc': pr_auc
            })

            # Calculate validation loss
            total_val_loss = loss_fn(torch.tensor(all_probs_array), torch.tensor(all_labels)).item()
            val_losses.append(total_val_loss / len(val_loader))

            # Early stopping logic
            if val_losses[-1] < best_val_loss:
                best_val_loss = val_losses[-1]
                epochs_no_improve = 0
                best_model_state = model.state_dict()  # Save the best model
            else:
                epochs_no_improve += 1
                if epochs_no_improve == patience:
                    print(f"Early stopping at epoch {epoch + 1}")
                    # Save early stopping message for this fold
                    with open(f"{output_dir}/early_stopping_messages.txt", "a") as f:
                        f.write(f"Seed {seed} - Fold {fold + 1}: Early stopping triggered at epoch {epoch + 1}\n")
                    break

            # Store losses
            train_losses.append(total_train_loss / len(train_loader))
        
        # Save fold results to CSV
        fold_metrics = pd.DataFrame(test_metrics)
        fold_metrics.to_csv(f"{output_dir}/seed{seed}_fold{fold+1}_metrics.csv", index=False)
        
        # Store fold results for combined metrics
        fold_results.append(test_metrics[-1])  # Store metrics for the last epoch of this fold

        # Save the best model for this fold
        torch.save(best_model_state, f"{output_dir}/seed{seed}_fold{fold+1}_best_model.pth")

        # Append losses and metrics for overall results
        all_train_losses.append(train_losses)
        all_val_losses.append(val_losses)
        all_test_metrics.append(test_metrics)
        
        # Also append for per-seed epoch metrics
        seed_train_losses.append(train_losses)
        seed_val_losses.append(val_losses)
        seed_test_metrics.append(test_metrics)
        
        # ===============================
        # New Code Block: Per Emotion Metrics Evaluation for this fold
        model.load_state_dict(best_model_state)
        model.eval()
        all_preds, all_labels, all_probs = [], [], []
        with torch.no_grad():
            for batch in val_loader:
                input_ids_batch, attention_masks_batch, labels_batch = [t.to(device) for t in batch]
                outputs = model(input_ids_batch, attention_mask=attention_masks_batch)
                probs = torch.softmax(outputs, dim=1)
                _, predicted = torch.max(outputs, 1)
                all_preds.extend(predicted.cpu().numpy())
                all_labels.extend(labels_batch.cpu().numpy())
                all_probs.extend(probs.cpu().numpy())
        all_preds = np.array(all_preds)
        all_labels = np.array(all_labels)
        all_probs_array = np.vstack(all_probs)
        
        fold_emotion_metrics = {}
        for emotion in range(7):
            binary_true = (all_labels == emotion).astype(int)
            binary_pred = (all_preds == emotion).astype(int)
            binary_prob = all_probs_array[:, emotion]
            acc = accuracy_score(binary_true, binary_pred)
            prec = precision_score(binary_true, binary_pred, zero_division=0)
            rec = recall_score(binary_true, binary_pred, zero_division=0)
            f1_sc = f1_score(binary_true, binary_pred, zero_division=0)
            try:
                roc_auc_em = roc_auc_score(binary_true, binary_prob)
            except ValueError:
                roc_auc_em = float('nan')
            precision_pr, recall_pr, _ = precision_recall_curve(binary_true, binary_prob, pos_label=1)
            pr_auc_em = auc(recall_pr, precision_pr)
            
            fold_emotion_metrics[emotion] = {
                'accuracy': acc,
                'precision': prec,
                'recall': rec,
                'f1': f1_sc,
                'roc_auc': roc_auc_em,
                'pr_auc': pr_auc_em
            }
        # Append the per-emotion metrics for this fold to the seed list
        seed_emotion_fold_metrics.append(fold_emotion_metrics)
        # ===============================
        
    # Save combined results for this seed
    seed_metrics = pd.DataFrame(fold_results)
    seed_metrics.to_csv(f"{output_dir}/seed{seed}_combined_metrics.csv", index=False)

    # ===============================
    # New Code Block: Aggregate and Save Per Emotion Metrics for this seed
    # Average metrics across folds for each emotion
    seed_emotion_avg = {}
    for emotion in range(7):
        acc_list = [fold_metric[emotion]['accuracy'] for fold_metric in seed_emotion_fold_metrics]
        prec_list = [fold_metric[emotion]['precision'] for fold_metric in seed_emotion_fold_metrics]
        rec_list = [fold_metric[emotion]['recall'] for fold_metric in seed_emotion_fold_metrics]
        f1_list = [fold_metric[emotion]['f1'] for fold_metric in seed_emotion_fold_metrics]
        roc_auc_list = [fold_metric[emotion]['roc_auc'] for fold_metric in seed_emotion_fold_metrics]
        pr_auc_list = [fold_metric[emotion]['pr_auc'] for fold_metric in seed_emotion_fold_metrics]
        
        seed_emotion_avg[emotion] = {
            'accuracy': np.mean(acc_list),
            'precision': np.mean(prec_list),
            'recall': np.mean(rec_list),
            'f1': np.mean(f1_list),
            'roc_auc': np.mean(roc_auc_list),
            'pr_auc': np.mean(pr_auc_list)
        }
    # Convert to DataFrame and save CSV file named "emotions per seed"
    emotion_metrics_df = pd.DataFrame.from_dict(seed_emotion_avg, orient='index')
    emotion_metrics_df.index.name = 'emotion'
    emotion_metrics_df.reset_index(inplace=True)
    emotion_metrics_df.to_csv(f"{output_dir}/emotions_per_seed_{seed}.csv", index=False)
    # ===============================
    
    # ===============================
    # New Code Block: Per-Seed Epoch Metrics
    # Ensure each fold's list has length num_epochs by padding with NaN if necessary
    for i in range(len(seed_train_losses)):
        if len(seed_train_losses[i]) < num_epochs:
            seed_train_losses[i].extend([float('nan')] * (num_epochs - len(seed_train_losses[i])))
        if len(seed_val_losses[i]) < num_epochs:
            seed_val_losses[i].extend([float('nan')] * (num_epochs - len(seed_val_losses[i])))
        if len(seed_test_metrics[i]) < num_epochs:
            seed_test_metrics[i].extend([{'accuracy': float('nan'),
                                          'precision': float('nan'),
                                          'recall': float('nan'),
                                          'f1': float('nan'),
                                          'roc_auc': float('nan'),
                                          'pr_auc': float('nan')
                                         }] * (num_epochs - len(seed_test_metrics[i])))

    # Calculate mean and standard deviation of metrics across folds for each epoch for this seed
    seed_epoch_metrics = []
    for epoch in range(num_epochs):
        train_losses_epoch = [loss[epoch] for loss in seed_train_losses]
        val_losses_epoch = [loss[epoch] for loss in seed_val_losses]
        test_accuracies_epoch = [m[epoch]['accuracy'] for m in seed_test_metrics]

        mean_train_loss = np.nanmean(train_losses_epoch)
        std_train_loss = np.nanstd(train_losses_epoch)
        mean_val_loss = np.nanmean(val_losses_epoch)
        std_val_loss = np.nanstd(val_losses_epoch)
        mean_test_accuracy = np.nanmean(test_accuracies_epoch)
        std_test_accuracy = np.nanstd(test_accuracies_epoch)

        seed_epoch_metrics.append({
            'Epoch': epoch + 1,
            'Mean Training Loss': mean_train_loss,
            'Std Training Loss': std_train_loss,
            'Mean Validation Loss': mean_val_loss,
            'Std Validation Loss': std_val_loss,
            'Mean Testing Accuracy': mean_test_accuracy,
            'Std Testing Accuracy': std_test_accuracy
        })

    # Convert to DataFrame and save per-seed epoch metrics CSV
    seed_epoch_metrics_df = pd.DataFrame(seed_epoch_metrics)
    seed_epoch_metrics_df.to_csv(f"{output_dir}/epoch_metrics_seed_{seed}.csv", index=False)
    # ===============================
    
    # Store seed results for overall analysis
    all_results.extend(fold_results)

# Ensure consistent epoch count by padding with NaN for overall metrics
max_epochs = num_epochs
for i in range(len(all_train_losses)):
    if len(all_train_losses[i]) < max_epochs:
        all_train_losses[i].extend([float('nan')] * (max_epochs - len(all_train_losses[i])))
    if len(all_val_losses[i]) < max_epochs:
        all_val_losses[i].extend([float('nan')] * (max_epochs - len(all_val_losses[i])))
    if len(all_test_metrics[i]) < max_epochs:
        all_test_metrics[i].extend([{'accuracy': float('nan'), 'precision': float('nan'), 'recall': float('nan'), 'f1': float('nan'), 'roc_auc': float('nan'), 'pr_auc': float('nan')}] * (max_epochs - len(all_test_metrics[i])))

# Calculate mean and standard deviation of metrics across all folds and seeds PER EPOCH
epoch_metrics = []

for epoch in range(max_epochs):
    # Extract metrics for the current epoch across all folds and seeds
    train_losses_epoch = [loss[epoch] for loss in all_train_losses]
    val_losses_epoch = [loss[epoch] for loss in all_val_losses]
    test_accuracies_epoch = [m[epoch]['accuracy'] for m in all_test_metrics]

    # Calculate mean and standard deviation for the current epoch
    mean_train_loss = np.nanmean(train_losses_epoch)
    std_train_loss = np.nanstd(train_losses_epoch)
    mean_val_loss = np.nanmean(val_losses_epoch)
    std_val_loss = np.nanstd(val_losses_epoch)
    mean_test_accuracy = np.nanmean(test_accuracies_epoch)
    std_test_accuracy = np.nanstd(test_accuracies_epoch)

    epoch_metrics.append({
        'Epoch': epoch + 1,
        'Mean Training Loss': mean_train_loss,
        'Std Training Loss': std_train_loss,
        'Mean Validation Loss': mean_val_loss,
        'Std Validation Loss': std_val_loss,
        'Mean Testing Accuracy': mean_test_accuracy,
        'Std Testing Accuracy': std_test_accuracy
    })

# Convert to DataFrame
epoch_metrics_df = pd.DataFrame(epoch_metrics)

# Save to CSV
epoch_metrics_df.to_csv(f"{output_dir}/epoch_metrics.csv", index=False)

# Print the DataFrame
print(epoch_metrics_df)

# Calculate mean and standard deviation of metrics across all folds and seeds
mean_train_losses = np.mean([loss[-1] for loss in all_train_losses], axis=0)
mean_val_losses = np.mean([loss[-1] for loss in all_val_losses], axis=0)
mean_test_metrics = {
    'accuracy': np.mean([m[-1]['accuracy'] for m in all_test_metrics]),
    'precision': np.mean([m[-1]['precision'] for m in all_test_metrics]),
    'recall': np.mean([m[-1]['recall'] for m in all_test_metrics]),
    'f1': np.mean([m[-1]['f1'] for m in all_test_metrics]),
    'roc_auc': np.mean([m[-1]['roc_auc'] for m in all_test_metrics]),
    'pr_auc': np.mean([m[-1]['pr_auc'] for m in all_test_metrics])
}

std_test_metrics = {
    'accuracy': np.std([m[-1]['accuracy'] for m in all_test_metrics]),
    'precision': np.std([m[-1]['precision'] for m in all_test_metrics]),
    'recall': np.std([m[-1]['recall'] for m in all_test_metrics]),
    'f1': np.std([m[-1]['f1'] for m in all_test_metrics]),
    'roc_auc': np.std([m[-1]['roc_auc'] for m in all_test_metrics]),
    'pr_auc': np.std([m[-1]['pr_auc'] for m in all_test_metrics])
}

# Print mean and standard deviation of metrics
print("Mean Train Loss:", mean_train_losses)
print("Mean Validation Loss:", mean_val_losses)
print("Mean Metrics:", mean_test_metrics)
print("Standard Deviation of Metrics:", std_test_metrics)

# Save mean metrics, train/validation losses, and standard deviation to a file
with open(f"{output_dir}/mean_metrics.txt", "w") as f:
    f.write("Mean Train Loss:\n")
    f.write(str(mean_train_losses) + "\n\n")
    f.write("Mean Validation Loss:\n")
    f.write(str(mean_val_losses) + "\n\n")
    f.write("Mean Metrics:\n")
    f.write(str(mean_test_metrics) + "\n\n")
    f.write("Standard Deviation of Metrics:\n")
    f.write(str(std_test_metrics))
